# 4 - Extracting value From Big Data With Queries

## Working With Pig

### Introduction

Hadoop est incroyablement puissant pour le traitement de grands ensembles de données
  - Nécessite des compétences en programmation
  - Les applications sont de faible niveau
  - Nécessite souvent un effort important pour développer des applications


Pig est une extension qui simplifie la programmation
  - Développé avec les objectifs d'être:
    - Facile à utiliser
    - Massivement évolutif

Il y a deux composants principaux de Cochon
  - Un langage de traitement de données de haut niveau appelé Pig Latin
  - Un compilateur qui traduit les programmes Pig Latin en Hadoop

### Pig Latin

Types de données
  - Les données d'entrée peuvent être n'importe quel format
  - Fonctionne sur des données relationnelles, semi-structurées ou non structurées
  - Prend en charge les types de données complexes, tels que les sacs et les tuples

Langage de flux de données
  - Exprime les exigences en tant qu'algorithmes
  - Chaque étape est une transformation de données
  - Prise en charge des opérateurs relationnels tels que filtre, union, groupe

Fonctions définies par l'utilisateur
  - Fournir un moyen de spécifier le traitement personnalisé
  - Main way Pig est extensible pour traiter n'importe quel type de données
  - Peut être implémenté en Java, Python, JavaScript

### Rinning Pig


Les commandes Pig Latin peuvent être exécutées de trois manières:
  - Coque interactive Grunt
  - Fichier de script
  - Intégré dans les programmes Java


Chacune des trois façons peut fonctionner dans l'un des deux modes
  - Mode local
    - Ceci est un mode de développement
    - N'utilise pas Hadoop
    - Fournit une rétroaction immédiate sur les scripts Pig
  - Mode Hadoop
    - Utilisé pour traiter de grands ensembles de données
    - Script Pig compilé dans le programme Hadoop
    - Distribue sur tous les nœuds du cluster Hadoop

### Grunt Interactive Shell

Active l'exécution des instructions Pig Latin
  - Fournit des commandes d'utilitaire de base pour interagir avec HDFS

Pour exécuter le shell Grunt en mode local:

```
pig –x local	
```

Grunt commands include
  - help
  - quit
  - set debug [on|off]
  - cat, cd , cp, ls, mkdir, mv, pwd, rm

### Basic Pig Latin

Pig ne chargera pas les données dans la mémoire
  - Pig analyse l'instruction, mais ne l'exécute que si nécessaire

La commande DUMP permet d'imprimer des données à l'écran
  - Utilisé pendant le développement
  - Seulement pour un sous-ensemble de données

### Processing Data

Pensez à savoir combien d'enregistrements chaque artiste a dans le fichier
Le script Pig Latin pour y parvenir est illustré ci-dessous:

  ````
  grunt> recordings = LOAD 'music_data.txt' USING PigStorage(',') AS 
  	(id:long, price:double, artist:chararray, title:chararray, 						duration:double, year:long);
  grunt> artists = GROUP recordings BY artist;
  grunt> result = FOREACH artists GENERATE recordings.artist, 							COUNT(recordings);
  grunt> STORE result INTO 'results';
  grunt> DUMP results;
  ````

### Comparing Pig Latin and SQL

Les principales différences entre Pig Latin et SQL sont
  - Pig Latin est un langage de traitement de données
    - Utilisé pour spécifier une série d'étapes de traitement
    - SQL définit des requêtes complexes avec des clauses
  - Pig Latin ne nécessite pas de schéma
    - Le schéma peut être défini lors du chargement des données
      - Comme dans l'exemple sur la diapositive 4-11
      - Le schéma ne doit pas être strictement respecté
    - SQL a toujours un schéma fixe

### Expressions and Functions

Sont appliqués aux champs de données pour calculer les valeurs
Les champs sont référencés par nom ou par index
  - L'index commence à 0 pour le premier champ

  ```
  grunt> enregistrements = LOAD 'music_data.txt' UTILISATION DE PigStorage (',')
  AS (id: long, prix: double, artiste: chararray,
  titre: chararray, durée: double, année: longue);
  ```

Dans l'exemple ci-dessus, il y a 6 champs, indexés de 0 à 5

### Relational Operators

Ce sont les opérateurs relationnels en latin Pig qui fournissent la puissance réelle
  - Utilisé en collaboration avec des expressions et des fonctions

Pour introduire ces opérateurs, nous utiliserons des données numériques dans les exemples
  - Rend l'utilisation des opérateurs plus claire

  ```
  grunt> a = LOAD 'file1.txt' USING PigStorage(',') AS (a1:int, a2:int, a3:int)
  grunt> b = LOAD 'file2.txt' USING PigStorage(',') AS (b1:int, b2:int, b3:int)
  
  grunt> DUMP a;
  (0,1,2) (2,4,6)
  grunt> DUMP b;
  (0,7,8) (1,9,10)
  grunt> c = UNION a,b
  grunt> DUMP c;
  (0,1,2) (2,4,6) (0,7,8) (1,9,10)
  ```
  
### FOREACH

Itère dans tout les tuples et génère de nouveaux tuples dans la sortie

  ```
  grunt> a = LOAD 'file1.txt' USING PigStorage(',') AS (a1:int, a2:int, a3:int)

  grunt> DUMP a;
  (0,1,2) (2,4,6)
  
  grunt> c = FOREACH a GENERATE a1, a2-a3;
  grunt> DUMP c;
  
  (0, -1)
  (2, -2)
  ```

### User-Defined Functions (UDF)

## HIVE

Un ensemble d'entreposage de données construit au-dessus de Hadoop

  - Initialement développé sur Facebook

Public cible pour Hive est des analystes de données à l'aise avec SQL

  - Fournit un accès à des requêtes ad hoc et à l'analyse de données
    - Sur les jeux de données à grande échelle

Se concentre sur les données structurées

  - Permet d'effectuer des optimisations

Fournit un langage de type SQL HiveQL

  - Basé sur des tables, des lignes, des colonnes
  - Pas besoin de savoir sur la programmation Hadoop

### Working with Hive

Hive nécessite un service métastore

  - Fournit des schémas pour structurer les données
    - Aide à l'interrogation et au traitement efficaces
  - Implémenté à l'aide de tables dans une base de données relationnelle
    - Par défaut, Hive utilise la base de données Derby intégrée de Java
    - Dans un environnement en cluster, la base de données autonome doit être configurée

La ruche est accessible depuis

  - Interface de ligne de commande
  - Interface Web connue sous le nom d'interface Web Hive

Hive a aussi le concept de seaux de données

  - Augmenter l'efficacité des requêtes sur des échantillons aléatoires de données
    - Par exemple, calculer le prix moyen des enregistrements
    - Un échantillon aléatoire de données peut fournir une bonne approximation
    - Pour l'enregistrement de musique, nous spécifions 32 seaux pour le prix
    - Le calcul du prix moyen ne nécessite que la numérisation 1/32 de tous les enregistrements

Hive est livré avec des connecteurs JDBC et ODBC

  - Permet aux tables du métastore Hive d'être accédées par des applications externes
  - Feuille de calcul Excel
  - Outils de Business Intelligence

## Comparing Pig and Hive

Pig and Hive semblent offrir des fonctionnalités similaires

Cependant, chacun a un rôle clair et distinct à jouer dans l'écosystème Hadoop

Pig est utilisé pour les tâches ETL
  - Lit généralement les données initialement introduites dans HDFS
  - Transforme les données dans un formulaire approprié pour l'analyse ad hoc
  - Exporte les données dans l'entrepôt de données Hive
  - Le processus s'exécute en traitement par lots
  - par exemple, Filtrer les données de journal Web
  - Éliminer les enregistrements de robot d'exploration (bot) Web

Hive est utilisée pour deux raisons :

  - En tant qu'entrepôt de données
  - Pour les requêtes ad hoc par les analystes métier

### Impala

Moteur Native Massively Parallel Processing (MPP) fonctionnant sous Hadoop
  - Développé par Cloudera mais open source
    - L'objectif était d'améliorer les performances de l'exécution de requêtes interactives

Permet de demander des données stockées dans HDFS
  - Aucun mouvement de données ou transformation
  - Prend en charge ANSI-92 SQL

Supporte ODBC et JDBC

  - Active l'intégration avec les outils de visualisation
  
par exemple, Tableau, Qlikview


