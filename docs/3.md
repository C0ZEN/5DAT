# 3 - Introduction to Hadoop

### Problématique

Lenteur de lecture des données (trop volumineux)  
Lorsque les disques sont volumineux

### Solution

Stocker les données sur plusieurs  
Accès en lecture beaucoup plus rapide  
Hadoop fournit la solution

### Hadoop

Permet donc de lire les disques  
De les associers  
Il permet également de faire des requêtes

Basé sur un système de nodes

### Origine

Yahoo avec Hadoop  
1000 coeurs (noeuds)  
Apache Top-Level (projet important)

Utilisé par :

- Facebook
- Last.fm
- DAF
- Google
- Amazon

### Ecosystème

Framework open-source  
Pour le traitement de données massives  
Fiable  
Scalable  
Couche abstraction

Pig

- Language pour requêter les dataset (abstraction)
- Select classique (mais pourtant requêtes complexes)

Hive

- Entrepot de données distribué sur plusieurs machines
- HQL (similaire au SQL, pour requête le datawerehouse)

HBase

- Répartion en column-family
- Sur un système distribué

Mahout

- Librairie de machine learning

Flume

- Live streaming

Oozie

- Programmation de tâches (workflow)
- Pour Hadoop (scipts)

### Example

Compter le nombre d'ocurrence de chaque mot dans un fichier texte

- Créer un compteur init à 0
- Découper le document pour chaque mot
- Pour chacun des mots, on incrémente le compteur
- Puis affiche la valeur du compteur

Faire ça sur une seule machine, c'est lent  
Sur des machines distribuées, le charge est répartie  
C'est beaucoup plus rapide parceque les traitements sont en parallèles

### Map reduce

Données à traiter en entrée  
Eclate un requêtage en X sous éléments  
Puis grouper par famille  
Fusionne les familles de chaque sous éléments  
Fusionne les familles pour avoir le résultat de la request

**Map :**

- Trie les données en catégories (plusieurs bloc de data)

**Shuffle :**

- Prend les catégories de chaque map
- Concatène en un résultat final

**Reduce :**

- Addition les catégories
- 1 seul résultat en sortie

### HDFS (Hadoop Distributed File System)

Les données sont stockées dans des "units" en tant que "blocks"  
De 64mb  
Une vidéo est donc généralement stockée sur plusieurs fichiers  
Et sur plusieurs nodes

Les nodes sont répliqués (haute dispo)

**Example :**

Un fichier  
Découpagé en 3 blocks  
Chaque block sera répartie sur 3 nodes

```
hadoop fs ...
```

### JobTracker Node

Reçoit les jobs des clients  
Gère les jobs  
Peut run plusieurs map ou tâches de réductions en parallèle  
Donne des infos sur l'état des tâches en cours

### NameNode

HDFS système master/slave  
Serveurs primaires/secondaires  
Track les fichiers  
1 seul par Hadoop cluster (ensemble de nodes)

**Example :**

NameNode XXX fait toutes les redirections vers les noeuds esclaves (nodes)

### Running Hadoop

- Local (pas de réplication)
    - Par défaut
    - N'utilise pas HDFS
    - Fait pour le debug
- Pseudo distribué
    - 1 seul cluster
    - 
    - 1 seule machine
    - Fait pour le debug
- Complètement distribué
    - Plusieurs cluster
    - Plusieurs machines
    - Fait pour la prod
    - Système master/slave

### Developing Haddop Appplication


Contexte : Pour comprendre ce qui peut être réalisé avec Hadoop, il faut:

  - Compréhension de la façon dont les données doivent être présentées à Hadoop
  - Comment Hadoop traite les données

Les diapositives suivantes affichent le code Java

  - L'instructeur expliquera le processus à un niveau élevé

Pour développer des applications Hadoop, quatre étapes majeures sont requises:
  - Analyse de données et formatage
  - Développer la fonction de carte
  - Développer la fonction de réduction
  - Exécutez le travail Hadoop

#### Étape 1: Analyse et formatage des données


Les entrées dans Hadoop doivent être dans des paires <key, value>

  - La plupart des données peuvent être structurées de cette manière
  - Les valeurs peuvent elles-mêmes être des collections (valeurs multiples)

Notre exemple utilisera des données d'enregistrement musical
  - L'ensemble de données échantillon contient des données d'enregistrement musical provenant d'un certain nombre de détaillants

#### Flux de données à travers Hadoop

L'étape de la carte extrait le titre, le prix en tant que paire <clé, valeur>
L'étape de mélange aléatoire regroupe toutes les paires <key, value> par clé
Réduire la scène trouve le prix 

#### Étape 2: Développer la fonction de carte

Nécessite une classe Java qui étend Mapper
  
  - Définir la méthode map ()

Reçoit la paire <key, value> de Hadoop
  
  - Extraire les données et transmettre la paire <key, value> à Hadoop
  - 

Exemple : 

  ```
  public class MaxRecordingPriceMapper extends 
  	Mapper<LongWritable, Text, Text, DoubleWritable> {
  
  
    @Override
    public void map(LongWritable key, Text value, Context context)
  	throws IOException, InterruptedException{
      	
  	String recordingText = value.toString();
  
  	String recordingTitle = …; // Extract title from input text
  	double price = …; // Extract price from input text 
     
  	context.write(new Text(recordingTitle), 
  					new DoubleWritable(price));
    }
  }
  ```

#### Étape 3: Développer la fonction Réduire

Nécessite une classe Java qui prolonge le réducteur
  
  - Définir la méthode reduce ()

**reduce ()** reçoit les paires <key, value>
  
  - Les valeurs sont des valeurs avec la même clé que celles combinées par réducteur

Renvoie la paire <key, value> avec le résultat
  
  - Dans notre exemple, le titre de l'enregistrement et le prix maximum pour cet enregistrement

Exemple :

  ```
  public class MaxRecordingPriceReducer extends 
  	Reducer<Text, DoubleWritable, Text, DoubleWritable> {
  
    @Override
    public void reduce(Text key, Iterable<DoubleWritable> values,
  						 Context context)
  			throws IOException, InterruptedException{
      	
  	double maximumPrice = 0.0; 
  	for(DoubleWritable value: values){
  	  maximumPrice = Math.max(maximumPrice, value.get());
  	}  
  	context.write(key,   new DoubleWritable(maximumPrice));
    }
  }
  ```
  
#### Étape 4: Exécutez le travail Hadoop

Nécessite la création d'un objet Job Hadoop
  
  - Fournit un contrôle sur l'exécution du travail

Le programme d'installation nécessite de spécifier
  
  - Chemin d'entrée pour le (s) fichier (s) de données à traiter
  - Type de classe de carte
  - Type de classe réducteur
    - Sortie <clé, valeur> types réducteur générera
  - Chemin de sortie pour les résultats à écrire

Une fois terminé, le travail est soumis à JobTracker

Exemple : 

  ```
  public class MaxRecordingPriceCalculator {
    public static void main(String [] args) throws Exception{
      Job job = new Job();
      job.setJarByClass(MaxRecordingPriceCalculator.class);
      job.setName("Max Recording Price");
      FileInputFormat.addInputPath(job, new Path(args[0]));
      FileOutputFormat.setOutputPath(job, new Path(args[1]));
   
      job.setMapperClass(MaxRecordingPriceMapper.class);
      job.setReducerClass(MaxRecordingPriceReducer.class);
  
      job.setOutputKeyClass(Text.class);
      job.setOutputValueClass(DoubleWritable.class);
  
      job.waitForCompletion(true);
    }
  }
  ```

### Activité: Conception de solutions Hadoop

Présenté avec un ensemble de données, votre tâche consiste à définir le travail qu'un mappeur et un réducteur doivent faire pour résoudre les problèmes informatiques suivants
 
  - L'objectif est d'aider à mieux comprendre comment Hadoop traite les données

Définissez les éléments suivants requis pour déterminer le prix moyen de tous les enregistrements de l'ensemble de données:
 
  - Paire clé-valeur d'entrée du mappeur
  - Mapper la paire clé-valeur de sortie
  - Réduction paire clé-valeur de sortie

Définissez les éléments suivants requis pour déterminer le nombre d'enregistrements (y compris les doublons) de chaque artiste dans l'ensemble de données:
  
  - Paire clé-valeur d'entrée du mappeur
  - Mapper la paire clé-valeur de sortie
  - Réduction paire clé-valeur de sortie

Définissez les éléments suivants requis pour déterminer l'enregistrement unique avec le prix le plus élevé dans l'ensemble de données:
  
  - Paire clé-valeur d'entrée du mappeur
  - Mapper la paire clé-valeur de sortie
  - Réduction paire clé-valeur de sortie

### Core Big Data Architecture

Rappelez les composants de base de l'architecture Big Data ci-dessous

  - HDFS fournit le stockage de données
  - MapReduce fournit un traitement par lots

